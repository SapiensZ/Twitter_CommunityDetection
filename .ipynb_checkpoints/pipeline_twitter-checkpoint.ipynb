{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import networkx as nx\n",
    "import plotly as py\n",
    "import plotly.graph_objects as go\n",
    "from plotly.graph_objs import *\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "from scipy.stats.stats import pearsonr\n",
    "import community\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_for_download(dates):\n",
    "    pwd = '/home/b00762654/twitter_None'\n",
    "    for i in range(len(dates) - 1):\n",
    "        csv_name = pwd + '/data_'+ str(dates[i]) + '_' + str(dates[i+1]) + '.csv'\n",
    "        print(csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dict of dataframe with twitter data from folder data/\n",
    "def load_folder_as_dict(dates, prefix='data/', subset=False, n_subset=100):\n",
    "    dict_df = dict()\n",
    "    for i in range(len(dates) - 1):\n",
    "        csv_name = prefix + 'data_'+ str(dates[i]) + '_' + str(dates[i+1]) + '.csv'\n",
    "        df_name = 'df_' + str(dates[i])\n",
    "        df = pd.read_csv(csv_name)\n",
    "        df = df[df.is_retweeted == True]\n",
    "        if subset:\n",
    "            df = df[:n_subset]\n",
    "        dict_df[df_name] = df\n",
    "    return dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output stats on dataframes --> to do with the whole dataset\n",
    "def get_stats_on_df(dict_df):\n",
    "    l_1 = []\n",
    "    l_2 = []\n",
    "\n",
    "    for key in dict_df.keys():\n",
    "        df = dict_df[key]\n",
    "        l_user_rt = list(df[df.is_retweeted == True].user_retweeted.unique())\n",
    "        l_users = list(df[df.is_retweeted == True].username.unique())\n",
    "        l_tot = l_users.copy()\n",
    "        print('DATE: ', key)\n",
    "        print('Nb unique user retweeted for ', key, ' : ', len(l_user_rt))\n",
    "        print('Nb unique users who tweet for ', key, ' : ', len(l_users))\n",
    "        l_tot.extend(l_user_rt)\n",
    "        print('Nb unique users for ', key, ' : ', len(list(set(l_tot))))\n",
    "        G = define_graph_from_df(df)\n",
    "        G_und = G.to_undirected()\n",
    "        print('Nodes in RT network:', len(G.nodes()))\n",
    "        print('Edges in RT network:', len(G.edges()))\n",
    "        num_of_cc = nx.number_connected_components(G_und)\n",
    "        print(\"Number of connected components: {}\".format(num_of_cc))\n",
    "        # Get the greatest connected component subgraph\n",
    "        gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "        gcc = G_und.subgraph(gcc_nodes)\n",
    "        print(\"Number of nodes in GCC: {}\".format(len(gcc_nodes)))\n",
    "        node_fraction = gcc.number_of_nodes() / float(G_und.number_of_nodes())\n",
    "        edge_fraction = gcc.number_of_edges() / float(G_und.number_of_edges())\n",
    "        print(\"Fraction of nodes in GCC: {:.3f}\".format(node_fraction))\n",
    "        print(\"Fraction of edges in GCC: {:.3f}\".format(edge_fraction))\n",
    "        print()\n",
    "        l_1.extend(l_user_rt)\n",
    "        l_2.extend(l_users)\n",
    "    print()\n",
    "    print('Nb of unique users retweeted for al these df :', len(list(set(l_1))))\n",
    "    print('Nb of unique users who tweet for al these df :', len(list(set(l_2))))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a directed graph from the datafrmae\n",
    "def define_graph_from_df(df):\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source = 'username', \n",
    "        target = 'user_retweeted',\n",
    "        create_using = nx.DiGraph())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionnary of directed graphs from a dictionnary of dataframe\n",
    "def define_graphs(dict_df):\n",
    "    graph_dict = dict()\n",
    "    for key in dict_df.keys():\n",
    "        graph_dict[key] = define_graph_from_df(dict_df[key])\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the max connected component in graph and dataframe format\n",
    "def get_df_G_connected_comp(G, df):\n",
    "    G_und = G.to_undirected()\n",
    "    gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "    df = df[(df.username.isin(list(gcc_nodes))) | (df.user_retweeted.isin(list(gcc_nodes)))]\n",
    "    gcc = define_graph_from_df(df)\n",
    "    return gcc, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a list of dataframes with connected components from the inital one (UNUSED)\n",
    "#Use it if you want to analyse different connected component inside 1 day\n",
    "def split_by_connected_components(G, df, limit=5):\n",
    "    G_und = G.to_undirected()\n",
    "    subdf_sorted = list()\n",
    "    gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "    while len(gcc_nodes) > limit:\n",
    "        df_res = df[(df.username.isin(list(gcc_nodes))) | (df.user_retweeted.isin(list(gcc_nodes)))]\n",
    "        subdf_sorted.append(df_res)\n",
    "        print(subdf_sorted[-1].shape)\n",
    "        G_und.remove_nodes_from(gcc_nodes)\n",
    "        gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "    return subdf_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return dictionnary of degree centrality for each node\n",
    "def compute_degree_centrality(G):\n",
    "    degree_centrality = {}\n",
    "    for node in G.nodes():\n",
    "        degree_centrality[node] = [G.degree(node),  G.in_degree(node), G.out_degree(node)]\n",
    "    return degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering of nodes\n",
    "#Louvain Clustering is included in that dataframe!\n",
    "def define_df_nodes(df, G):\n",
    "    \n",
    "    df_nodes = df[df.is_retweeted == True]\n",
    "    df_nodes.loc[:, 'count_rt_day'] = df_nodes.groupby('user_retweeted').user_retweeted.transform('count')\n",
    "    df_nodes.loc[:, 'count_t_day'] = df_nodes.groupby('username').username.transform('count')\n",
    "\n",
    "    df_nodes_1 = df_nodes[['user_retweeted', 'count_rt_day']].drop_duplicates()\n",
    "    df_nodes_1.columns = ['username', 'count_rt_day']\n",
    "    df_nodes_2 = df_nodes[['username', 'count_t_day']].drop_duplicates()\n",
    "    \n",
    "    df = pd.merge(df_nodes_1, df_nodes_2, on='username', how='outer')\n",
    "    #compute closeness centrality for each nodes\n",
    "    df_closeness = pd.DataFrame(nx.closeness_centrality(G).items(), columns=['username', 'closeness_centrality'])\n",
    "    #compute harmonic centrality for each nodes --> same results as above\n",
    "    #df_harmonic = pd.DataFrame(nx.harmonic_centrality(G).items(), columns=['username', 'harmonic_centrality'])\n",
    "    #compute betweenness centrality for each nodes\n",
    "    df_betweenness = pd.DataFrame(nx.betweenness_centrality(G).items(), columns=['username', 'betweenness_centrality'])\n",
    "    #compute the degree centrality for each nodes\n",
    "    df_degree = pd.DataFrame(compute_degree_centrality(G)).T.reset_index()\n",
    "    df_degree.columns = ['username', 'degree', 'in_degree', 'out_degree']  \n",
    "    #computer pagerank importance\n",
    "    df_pagerank = pd.DataFrame(nx.pagerank(G, alpha=0.9).items(), columns=['username', 'pagerank_value'])\n",
    "    \n",
    "    #define cluster_id according to Louvain method\n",
    "    G_und = G.to_undirected()\n",
    "    partition = community.best_partition(G_und)\n",
    "    df_clusters = pd.DataFrame(partition.items(), columns=['username', 'cluster_id'])\n",
    "    \n",
    "    \n",
    "    df = pd.merge(df, df_closeness, on='username', how='outer')\n",
    "    #df = pd.merge(df, df_harmonic, on='username', how='outer')\n",
    "    df = pd.merge(df, df_degree, on='username', how='outer')\n",
    "    df = pd.merge(df, df_betweenness, on='username', how='outer')\n",
    "    df = pd.merge(df, df_pagerank, on='username', how='outer')\n",
    "    df = pd.merge(df, df_clusters, on='username', how='outer')\n",
    "    \n",
    "    df.loc[:, 'modularity'] = community.modularity(partition, G_und)\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edges Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering of Edges\n",
    "def define_df_edges(df, G):\n",
    "    df_edges = df[['username', 'user_retweeted', 'timestamp', 'count_rt', 'text']].copy()\n",
    "    df_edges.loc[:, 'count_rt_day'] = df_edges.groupby('user_retweeted').user_retweeted.transform('count')\n",
    "    df_edges.loc[:, 'count_t_day'] = df_edges.groupby('username').username.transform('count')\n",
    "    \n",
    "    df_edges.loc[:, 'count_t_day'] = df_edges.groupby('username').username.transform('count')\n",
    "    \n",
    "    return df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_edges.sort_values('count_t_day', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering And Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two type of partition format:\n",
    "- list format : [{nodeset_cluster1}, {nodeset_cluster2}, ...]\n",
    "- dict format: {node1:cluser_id1, node2:cluster_id2 ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Less efficient than louvain clustering\n",
    "def spectral_clustering(G, k=2):\n",
    "    nodelist = list(G)\n",
    "    adj_mat = nx.to_numpy_matrix(G)\n",
    "    sc = SpectralClustering(k, affinity='precomputed', n_init=100, )\n",
    "    sc.fit(adj_mat)\n",
    "\n",
    "    labels = sc.labels_\n",
    "    \n",
    "    partition = [set() for _ in range(k)]\n",
    "    for i in range(len(nodelist)):\n",
    "        partition[labels[i]].add(nodelist[i])\n",
    "        \n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def louvain_clustering(G):\n",
    "    G_und = G.to_undirected()\n",
    "    partition = community.best_partition(G_und)\n",
    "    keys = list(set(partition.values()))\n",
    "    data = {key: [] for key in keys}\n",
    "    for node, commId in partition.items():\n",
    "        data[commId].append(node)\n",
    "    part_fin = []\n",
    "    for nodeset in data.values():\n",
    "        part_fin.append(set(nodeset))\n",
    "    return part_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(G, partition, save_name):\n",
    "    colors=[]\n",
    "    plt.figure(figsize=(20,15))\n",
    "    for i in range(len(partition)):\n",
    "        colors.append(cm.Set1(i))\n",
    "    pos=nx.spring_layout(G) # positions for all nodes\n",
    "    \n",
    "    for commId, nodeset in enumerate(partition):\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodeset, node_color=colors[commId],\n",
    "                               node_size=100, alpha=0.4)\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.1)\n",
    "    plt.savefig(save_name)\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "\n",
    "def visualize_clusters_labels(G, partition, dict_lab, save_name):\n",
    "    colors=[]\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(len(partition)):\n",
    "        colors.append(cm.Set1(i))\n",
    "    pos=nx.spring_layout(G) # positions for all nodes\n",
    "    \n",
    "    for commId, nodeset in enumerate(partition):\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodeset, node_color=colors[commId],\n",
    "                               node_size=100, alpha=0.3)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=dict_lab, font_size=15, font_weight='bold')\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.1)\n",
    "    plt.savefig(save_name)\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(G, partition):\n",
    "    G_und = G.to_undirected()\n",
    "    dict_partition = dict()\n",
    "    for i in range(len(partition)):\n",
    "        for node in partition[i]:\n",
    "            dict_partition[node] = i\n",
    "    modularity = community.modularity(dict_partition, G_und)\n",
    "    return modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_communities_with_limit(partition, limit_agg=10):\n",
    "    new_part = []\n",
    "    nodes_without_com = []\n",
    "    for i in range(len(part)):\n",
    "        if len(part[i]) > limit_agg:\n",
    "            new_part.append(part[i])\n",
    "        else:\n",
    "            nodes_without_com += list(part[i])\n",
    "    new_part.append(set(nodes_without_com))\n",
    "    return new_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two type of clustering format (see above), returns list format from dict format\n",
    "def change_format_clustering(partition_dict):\n",
    "    keys = list(set(partition_dict.values()))\n",
    "    data = {key: [] for key in keys}\n",
    "    for node, commId in partition_dict.items():\n",
    "        data[commId].append(node)\n",
    "    part_fin = []\n",
    "    for nodeset in data.values():\n",
    "        part_fin.append(set(nodeset))\n",
    "    return part_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df(df_nodes, G, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "\n",
    "    return visualize_clusters(G, partition_list, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df_closeness(df_nodes, G, top=10, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "    \n",
    "    #node information\n",
    "    nodes_labeled = df_nodes.sort_values('closeness_centrality', ascending=False).username[:top]\n",
    "    dict_nodes_labeled = dict(zip(nodes_labeled, nodes_labeled))\n",
    "    \n",
    "    return visualize_clusters_labels(G, partition_list, dict_nodes_labeled, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df_pagerank(df_nodes, G, top=10, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "    \n",
    "    #node information\n",
    "    nodes_labeled = df_nodes.sort_values('pagerank_value', ascending=False).username[:top]\n",
    "    dict_nodes_labeled = dict(zip(nodes_labeled, nodes_labeled))\n",
    "    \n",
    "    return visualize_clusters_labels(G, partition_list, dict_nodes_labeled, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df_betweenness(df_nodes, G, top=10, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "    \n",
    "    #node information\n",
    "    nodes_labeled = df_nodes.sort_values('betweenness_centrality', ascending=False).username[:top]\n",
    "    dict_nodes_labeled = dict(zip(nodes_labeled, nodes_labeled))\n",
    "    \n",
    "    return visualize_clusters_labels(G, partition_list, dict_nodes_labeled, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_top_clusters(df_nodes, top_clusters):\n",
    "    s_top_clusters = df_nodes.groupby('cluster_id').cluster_id.count().sort_values(ascending=False)[:top_clusters]\n",
    "    l_top_clusters = list(s_top_clusters.index)\n",
    "    other_cluster_id = random.choice([x for x in range(999) if x not in l_top_clusters])\n",
    "    \n",
    "    def reassign_clusters(cluster_id):\n",
    "        if cluster_id not in l_top_clusters:\n",
    "            return other_cluster_id\n",
    "        else:\n",
    "            return cluster_id\n",
    "    \n",
    "    df_nodes.loc[:, 'new_cluster_id'] = df_nodes.cluster_id.apply(reassign_clusters)\n",
    "    return df_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31',\n",
    "        '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04',\n",
    "        '2020-04-05', '2020-04-06', '2020-04-07', '2020-04-08',\n",
    "         '2020-04-09', '2020-04-10']\n",
    "#dates= ['2020-04-08', '2020-04-09']\n",
    "top_users_display = 10\n",
    "filter_by_top_cluster = True\n",
    "top_clusters_display=10\n",
    "param_subset=True\n",
    "param_n_subset=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of dates: ['2020-04-08', '2020-04-09']\n",
      "Date:  2020-04-08\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (sec): 2.180330991744995\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    df_dict = load_folder_as_dict(dates, prefix='../data/', subset=param_subset, n_subset=param_n_subset)\n",
    "    graph_dict = define_graphs(df_dict)\n",
    "    print('Range of dates:', dates)\n",
    "\n",
    "    for key in list(df_dict.keys()):\n",
    "        t1 = time.time()\n",
    "        print('Date: ', str(key).split('_')[-1])\n",
    "\n",
    "        df = df_dict[key]\n",
    "        G = graph_dict[key]\n",
    "\n",
    "        #Keep connected component\n",
    "        G, df = get_df_G_connected_comp(G, df)\n",
    "        G_und = G.to_undirected()\n",
    "\n",
    "        #define df_nodes\n",
    "        df_nodes = define_df_nodes(df, G)\n",
    "        #Reassign top cluster for a better display\n",
    "        if filter_by_top_cluster:\n",
    "            df_nodes = reassign_top_clusters(df_nodes, top_clusters=top_clusters_display)\n",
    "            df_nodes.loc[:, 'cluster_id'] = df_nodes.loc[:, 'new_cluster_id']\n",
    "        print(df_nodes.cluster_id.nunique())\n",
    "        print(df_nodes.new_cluster_id.nunique())\n",
    "        savename_nodes  = '../individual_data/processed_data/nodes/' + str(key).split('_')[-1]+'_nodes.csv'\n",
    "        df_nodes.to_csv(savename_nodes,  encoding='utf-8')\n",
    "        \n",
    "        #define df_edges\n",
    "        df_edges = define_df_edges(df, G)\n",
    "        savename_edges  = '../individual_data/processed_data/edges/' + str(key).split('_')[-1]+'_edges.csv'\n",
    "        df_edges.to_csv(savename_edges,  encoding='utf-8')\n",
    "\n",
    "        #compute and save figures according to clustering and (top) most important nodes\n",
    "        savename_clusters  = '../individual_data/visualisation/clusters/' + str(key).split('_')[-1]+'_clusters'\n",
    "        vizualize_from_df(df_nodes, G_und, savename=savename_clusters)\n",
    "        savename_clusters  = '../individual_data/visualisation/betweenness/' + str(key).split('_')[-1]+'_betweenness'\n",
    "        vizualize_from_df_betweenness(df_nodes, G_und, top=top_users_display ,savename=savename_clusters)\n",
    "        savename_clusters  = '../individual_data/visualisation/closeness/' + str(key).split('_')[-1]+'_closeness'\n",
    "        vizualize_from_df_closeness(df_nodes, G_und, top=top_users_display ,savename=savename_clusters)\n",
    "        savename_clusters  = '../individual_data/visualisation/pagerank/' + str(key).split('_')[-1]+'_pagerank'\n",
    "        vizualize_from_df_pagerank(df_nodes, G_und, top=top_users_display, savename=savename_clusters)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        delta_t = t2-t1\n",
    "        print('Time taken (sec):', delta_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
