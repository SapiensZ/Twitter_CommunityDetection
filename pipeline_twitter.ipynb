{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import networkx as nx\n",
    "import plotly as py\n",
    "import plotly.graph_objects as go\n",
    "from plotly.graph_objs import *\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "from scipy.stats.stats import pearsonr\n",
    "import community\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API credentials\n",
    "#consumer_key = \"BmvoUUcOUXhPxRR8uRC2TgKoW\"\n",
    "#consumer_secret = \"bNV6inRgeUSSVerytnnnTPveW8iM9GM0dwryZyiUKmYy436D1I\"\n",
    "#access_key = \"2969993776-b9Ui7fVJjW7gYId2C0kSGo5mN4ki93HSGEn6jx0\"\n",
    "#access_secret = \"N5ER33zjeIqfl5918MWTHLWbZzuBGfGL0FeSfNGvSsrvZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OAUTH_KEYS = {'consumer_key':consumer_key, 'consumer_secret':consumer_secret, 'access_token_key':access_key, 'access_token_secret':access_secret}\n",
    "#auth = tweepy.OAuthHandler(OAUTH_KEYS['consumer_key'], OAUTH_KEYS['consumer_secret'])\n",
    "#api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_for_download(dates):\n",
    "    pwd = '/home/b00762654/twitter_None'\n",
    "    for i in range(len(dates) - 1):\n",
    "        csv_name = pwd + '/data_'+ str(dates[i]) + '_' + str(dates[i+1]) + '.csv'\n",
    "        print(csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_for_download(dates):\n",
    "    pwd = '/home/b00762654/twitter_None'\n",
    "    for i in range(len(dates) - 1):\n",
    "        csv_name = pwd + '/data_'+ str(dates[i]) + '_' + str(dates[i+1]) + '.csv'\n",
    "        print(csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dict of dataframe with twitter data from folder data/\n",
    "def load_folder_as_dict(dates, prefix='data/', subset=False, n_subset=100):\n",
    "    dict_df = dict()\n",
    "    for i in range(len(dates) - 1):\n",
    "        csv_name = prefix + 'data_'+ str(dates[i]) + '_' + str(dates[i+1]) + '.csv'\n",
    "        df_name = 'df_' + str(dates[i])\n",
    "        df = pd.read_csv(csv_name)\n",
    "        df = df[df.is_retweeted == True]\n",
    "        if subset:\n",
    "            df = df[:n_subset]\n",
    "        dict_df[df_name] = df\n",
    "    return dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output stats on dataframes --> to do with the whole dataset\n",
    "def get_stats_on_df(dict_df):\n",
    "    l_1 = []\n",
    "    l_2 = []\n",
    "\n",
    "    for key in dict_df.keys():\n",
    "        df = dict_df[key]\n",
    "        l_user_rt = list(df[df.is_retweeted == True].user_retweeted.unique())\n",
    "        l_users = list(df[df.is_retweeted == True].username.unique())\n",
    "        l_tot = l_users.copy()\n",
    "        print('DATE: ', key)\n",
    "        print('Nb unique user retweeted for ', key, ' : ', len(l_user_rt))\n",
    "        print('Nb unique users who tweet for ', key, ' : ', len(l_users))\n",
    "        l_tot.extend(l_user_rt)\n",
    "        print('Nb unique users for ', key, ' : ', len(list(set(l_tot))))\n",
    "        G = define_graph_from_df(df)\n",
    "        G_und = G.to_undirected()\n",
    "        print('Nodes in RT network:', len(G.nodes()))\n",
    "        print('Edges in RT network:', len(G.edges()))\n",
    "        num_of_cc = nx.number_connected_components(G_und)\n",
    "        print(\"Number of connected components: {}\".format(num_of_cc))\n",
    "        # Get the greatest connected component subgraph\n",
    "        gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "        gcc = G_und.subgraph(gcc_nodes)\n",
    "        print(\"Number of nodes in GCC: {}\".format(len(gcc_nodes)))\n",
    "        node_fraction = gcc.number_of_nodes() / float(G_und.number_of_nodes())\n",
    "        edge_fraction = gcc.number_of_edges() / float(G_und.number_of_edges())\n",
    "        print(\"Fraction of nodes in GCC: {:.3f}\".format(node_fraction))\n",
    "        print(\"Fraction of edges in GCC: {:.3f}\".format(edge_fraction))\n",
    "        print()\n",
    "        l_1.extend(l_user_rt)\n",
    "        l_2.extend(l_users)\n",
    "    print()\n",
    "    print('Nb of unique users retweeted for al these df :', len(list(set(l_1))))\n",
    "    print('Nb of unique users who tweet for al these df :', len(list(set(l_2))))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a directed graph from the datafrmae\n",
    "def define_graph_from_df(df):\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source = 'username', \n",
    "        target = 'user_retweeted',\n",
    "        create_using = nx.DiGraph())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionnary of directed graphs from a dictionnary of dataframe\n",
    "def define_graphs(dict_df):\n",
    "    graph_dict = dict()\n",
    "    for key in dict_df.keys():\n",
    "        graph_dict[key] = define_graph_from_df(dict_df[key])\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the max connected component in graph and dataframe format\n",
    "def get_df_G_connected_comp(G, df):\n",
    "    G_und = G.to_undirected()\n",
    "    gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "    df = df[(df.username.isin(list(gcc_nodes))) | (df.user_retweeted.isin(list(gcc_nodes)))]\n",
    "    gcc = define_graph_from_df(df)\n",
    "    return gcc, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a list of dataframes with connected components from the inital one (UNUSED)\n",
    "#Use it if you want to analyse different connected component inside 1 day\n",
    "def split_by_connected_components(G, df, limit=5):\n",
    "    G_und = G.to_undirected()\n",
    "    subdf_sorted = list()\n",
    "    gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "    while len(gcc_nodes) > limit:\n",
    "        df_res = df[(df.username.isin(list(gcc_nodes))) | (df.user_retweeted.isin(list(gcc_nodes)))]\n",
    "        subdf_sorted.append(df_res)\n",
    "        print(subdf_sorted[-1].shape)\n",
    "        G_und.remove_nodes_from(gcc_nodes)\n",
    "        gcc_nodes = max(nx.connected_components(G_und), key=len)\n",
    "    return subdf_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return dictionnary of degree centrality for each node\n",
    "def compute_degree_centrality(G):\n",
    "    degree_centrality = {}\n",
    "    for node in G.nodes():\n",
    "        degree_centrality[node] = [G.degree(node),  G.in_degree(node), G.out_degree(node)]\n",
    "    return degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering of nodes\n",
    "#Louvain Clustering is included in that dataframe!\n",
    "def define_df_nodes(df, G):\n",
    "    \n",
    "    df_nodes = df[df.is_retweeted == True]\n",
    "    df_nodes.loc[:, 'count_rt_day'] = df_nodes.groupby('user_retweeted').user_retweeted.transform('count')\n",
    "    df_nodes.loc[:, 'count_t_day'] = df_nodes.groupby('username').username.transform('count')\n",
    "\n",
    "    df_nodes_1 = df_nodes[['user_retweeted', 'count_rt_day']].drop_duplicates()\n",
    "    df_nodes_1.columns = ['username', 'count_rt_day']\n",
    "    df_nodes_2 = df_nodes[['username', 'count_t_day']].drop_duplicates()\n",
    "    \n",
    "    df = pd.merge(df_nodes_1, df_nodes_2, on='username', how='outer')\n",
    "    #compute closeness centrality for each nodes\n",
    "    df_closeness = pd.DataFrame(nx.closeness_centrality(G).items(), columns=['username', 'closeness_centrality'])\n",
    "    #compute harmonic centrality for each nodes --> same results as above\n",
    "    #df_harmonic = pd.DataFrame(nx.harmonic_centrality(G).items(), columns=['username', 'harmonic_centrality'])\n",
    "    #compute betweenness centrality for each nodes\n",
    "    df_betweenness = pd.DataFrame(nx.betweenness_centrality(G).items(), columns=['username', 'betweenness_centrality'])\n",
    "    #compute the degree centrality for each nodes\n",
    "    df_degree = pd.DataFrame(compute_degree_centrality(G)).T.reset_index()\n",
    "    df_degree.columns = ['username', 'degree', 'in_degree', 'out_degree']  \n",
    "    #computer pagerank importance\n",
    "    df_pagerank = pd.DataFrame(nx.pagerank(G, alpha=0.9).items(), columns=['username', 'pagerank_value'])\n",
    "    \n",
    "    #define cluster_id according to Louvain method\n",
    "    G_und = G.to_undirected()\n",
    "    partition = community.best_partition(G_und)\n",
    "    df_clusters = pd.DataFrame(partition.items(), columns=['username', 'cluster_id'])\n",
    "    \n",
    "    \n",
    "    df = pd.merge(df, df_closeness, on='username', how='outer')\n",
    "    #df = pd.merge(df, df_harmonic, on='username', how='outer')\n",
    "    df = pd.merge(df, df_degree, on='username', how='outer')\n",
    "    df = pd.merge(df, df_betweenness, on='username', how='outer')\n",
    "    df = pd.merge(df, df_pagerank, on='username', how='outer')\n",
    "    df = pd.merge(df, df_clusters, on='username', how='outer')\n",
    "    \n",
    "    df.loc[:, 'modularity'] = community.modularity(partition, G_und)\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edges Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering of Edges\n",
    "def define_df_edges(df, G):\n",
    "    df_edges = df[['username', 'user_retweeted', 'timestamp', 'count_rt', 'text']].copy()\n",
    "    df_edges.loc[:, 'count_rt_day'] = df_edges.groupby('user_retweeted').user_retweeted.transform('count')\n",
    "    df_edges.loc[:, 'count_t_day'] = df_edges.groupby('username').username.transform('count')\n",
    "    \n",
    "    df_edges.loc[:, 'count_t_day'] = df_edges.groupby('username').username.transform('count')\n",
    "    \n",
    "    return df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_edges.sort_values('count_t_day', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering And Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two type of partition format:\n",
    "- list format : [{nodeset_cluster1}, {nodeset_cluster2}, ...]\n",
    "- dict format: {node1:cluser_id1, node2:cluster_id2 ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Less efficient than louvain clustering\n",
    "def spectral_clustering(G, k=2):\n",
    "    nodelist = list(G)\n",
    "    adj_mat = nx.to_numpy_matrix(G)\n",
    "    sc = SpectralClustering(k, affinity='precomputed', n_init=100, )\n",
    "    sc.fit(adj_mat)\n",
    "\n",
    "    labels = sc.labels_\n",
    "    \n",
    "    partition = [set() for _ in range(k)]\n",
    "    for i in range(len(nodelist)):\n",
    "        partition[labels[i]].add(nodelist[i])\n",
    "        \n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def louvain_clustering(G):\n",
    "    G_und = G.to_undirected()\n",
    "    partition = community.best_partition(G_und)\n",
    "    keys = list(set(partition.values()))\n",
    "    data = {key: [] for key in keys}\n",
    "    for node, commId in partition.items():\n",
    "        data[commId].append(node)\n",
    "    part_fin = []\n",
    "    for nodeset in data.values():\n",
    "        part_fin.append(set(nodeset))\n",
    "    return part_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(G, partition, save_name):\n",
    "    colors=[]\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(len(partition)):\n",
    "        colors.append(cm.Set1(i))\n",
    "    pos=nx.spring_layout(G) # positions for all nodes\n",
    "    \n",
    "    for commId, nodeset in enumerate(partition):\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodeset, node_color=colors[commId],\n",
    "                               node_size=100, alpha=0.4)\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    plt.savefig(save_name)\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters_labels(G, partition, dict_lab, save_name):\n",
    "    colors=[]\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(len(partition)):\n",
    "        colors.append(cm.Set1(i))\n",
    "    pos=nx.spring_layout(G) # positions for all nodes\n",
    "    \n",
    "    for commId, nodeset in enumerate(partition):\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodeset, node_color=colors[commId],\n",
    "                               node_size=100, alpha=0.4)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=dict_lab)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    plt.savefig(save_name)\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(G, partition):\n",
    "    G_und = G.to_undirected()\n",
    "    dict_partition = dict()\n",
    "    for i in range(len(partition)):\n",
    "        for node in partition[i]:\n",
    "            dict_partition[node] = i\n",
    "    modularity = community.modularity(dict_partition, G_und)\n",
    "    return modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_communities_with_limit(partition, limit_agg=10):\n",
    "    new_part = []\n",
    "    nodes_without_com = []\n",
    "    for i in range(len(part)):\n",
    "        if len(part[i]) > limit_agg:\n",
    "            new_part.append(part[i])\n",
    "        else:\n",
    "            nodes_without_com += list(part[i])\n",
    "    new_part.append(set(nodes_without_com))\n",
    "    return new_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two type of clustering format (see above), returns list format from dict format\n",
    "def change_format_clustering(partition_dict):\n",
    "    keys = list(set(partition_dict.values()))\n",
    "    data = {key: [] for key in keys}\n",
    "    for node, commId in partition_dict.items():\n",
    "        data[commId].append(node)\n",
    "    part_fin = []\n",
    "    for nodeset in data.values():\n",
    "        part_fin.append(set(nodeset))\n",
    "    return part_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df(df_nodes, G, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "\n",
    "    return visualize_clusters(G, partition_list, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df_closeness(df_nodes, G, top=10, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "    \n",
    "    #node information\n",
    "    nodes_labeled = df_nodes.sort_values('closeness_centrality', ascending=False).username[:top]\n",
    "    dict_nodes_labeled = dict(zip(nodes_labeled, nodes_labeled))\n",
    "    \n",
    "    return visualize_clusters_labels(G, partition_list, dict_nodes_labeled, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df_pagerank(df_nodes, G, top=10, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "    \n",
    "    #node information\n",
    "    nodes_labeled = df_nodes.sort_values('pagerank_value', ascending=False).username[:top]\n",
    "    dict_nodes_labeled = dict(zip(nodes_labeled, nodes_labeled))\n",
    "    \n",
    "    return visualize_clusters_labels(G, partition_list, dict_nodes_labeled, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_from_df_betweenness(df_nodes, G, top=10, savename='test'):\n",
    "    #G_und = G.to_undirected(G)\n",
    "    partition_dict = dict(zip(df_nodes.username, df_nodes.cluster_id))\n",
    "    partition_list = change_format_clustering(partition_dict)\n",
    "    \n",
    "    #node information\n",
    "    nodes_labeled = df_nodes.sort_values('betweenness_centrality', ascending=False).username[:top]\n",
    "    dict_nodes_labeled = dict(zip(nodes_labeled, nodes_labeled))\n",
    "    \n",
    "    return visualize_clusters_labels(G, partition_list, dict_nodes_labeled, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31',\n",
    "        '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04',\n",
    "        '2020-04-05', '2020-04-06', '2020-04-07', '2020-04-08',\n",
    "         '2020-04-09', '2020-04-10']\n",
    "dates= dates[:3]\n",
    "top_n =10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31',\n",
    "        '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04',\n",
    "        '2020-04-05', '2020-04-06', '2020-04-07', '2020-04-08',\n",
    "         '2020-04-09', '2020-04-10']\n",
    "dates= dates[:3]\n",
    "top_n =10\n",
    "param_subset=False\n",
    "param_n_subset=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of dates: ['2020-03-28', '2020-03-29', '2020-03-30']\n",
      "Date:  2020-03-28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-f54e58ac3ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#define df_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdf_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_df_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0msavename_nodes\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m'processed_data/nodes/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_nodes.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdf_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavename_nodes\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-10b9c5452e46>\u001b[0m in \u001b[0;36mdefine_df_nodes\u001b[0;34m(df, G)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#define cluster_id according to Louvain method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mG_und\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_undirected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommunity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_und\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mdf_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ipseite/lib/python3.7/site-packages/community/community_louvain.py\u001b[0m in \u001b[0;36mbest_partition\u001b[0;34m(graph, partition, weight, resolution, randomize, random_state)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                 \u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                                 \u001b[0mrandomize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                                 random_state)\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpartition_at_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdendo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdendo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ipseite/lib/python3.7/site-packages/community/community_louvain.py\u001b[0m in \u001b[0;36mgenerate_dendrogram\u001b[0;34m(graph, part_init, weight, resolution, randomize, random_state)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mstatus_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0m__one_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mnew_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__modularity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__renumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode2com\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ipseite/lib/python3.7/site-packages/community/community_louvain.py\u001b[0m in \u001b[0;36m__one_level\u001b[0;34m(graph, status, weight_key, resolution, random_state)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mbest_com\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mbest_increase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_communities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mincr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_cost\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresolution\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdnc\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                        \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdegc_totw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    df_dict = load_folder_as_dict(dates, prefix='data/', subset=param_subset, n_subset=param_n_subset)\n",
    "    graph_dict = define_graphs(df_dict)\n",
    "    print('Range of dates:', dates)\n",
    "\n",
    "    for key in list(df_dict.keys()):\n",
    "        t1 = time.time()\n",
    "        print('Date: ', str(key).split('_')[-1])\n",
    "\n",
    "        df = df_dict[key]\n",
    "        G = graph_dict[key]\n",
    "\n",
    "        #Keep connected component\n",
    "        G, df = get_df_G_connected_comp(G, df)\n",
    "        G_und = G.to_undirected()\n",
    "\n",
    "        #define df_nodes\n",
    "        df_nodes = define_df_nodes(df, G)\n",
    "        savename_nodes  = 'processed_data/nodes/' + str(key).split('_')[-1]+'_nodes.csv'\n",
    "        df_nodes.to_csv(savename_nodes,  encoding='utf-8')\n",
    "\n",
    "        #define df_edges\n",
    "        df_edges = define_df_edges(df, G)\n",
    "        savename_edges  = 'processed_data/edges/' + str(key).split('_')[-1]+'_edges.csv'\n",
    "        df_edges.to_csv(savename_edges,  encoding='utf-8')\n",
    "\n",
    "        #compute and save figures according to clustering and (top) most important nodes\n",
    "        savename_clusters  = 'visualisation/clusters/' + str(key).split('_')[-1]+'_clusters'\n",
    "        vizualize_from_df(df_nodes, G, savename=savename_clusters)\n",
    "        savename_clusters  = 'visualisation/betweenness/' + str(key).split('_')[-1]+'_betweenness'\n",
    "        vizualize_from_df_betweenness(df_nodes, G, top=top_n ,savename=savename_clusters)\n",
    "        savename_clusters  = 'visualisation/closeness/' + str(key).split('_')[-1]+'_closeness'\n",
    "        vizualize_from_df_closeness(df_nodes, G, top=top_n ,savename=savename_clusters)\n",
    "        savename_clusters  = 'visualisation/pagerank/' + str(key).split('_')[-1]+'_pagerank'\n",
    "        vizualize_from_df_pagerank(df_nodes, G, top=top_n, savename=savename_clusters)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        delta_t = t2-t1\n",
    "        print('Time taken (sec):', delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
